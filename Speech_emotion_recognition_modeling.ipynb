{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Speech_emotion_recognition_modeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTzJxW7bjJScVQ1v30W5tB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shadfdz/speechEmotionRecognition/blob/master/Speech_emotion_recognition_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8df-NrCbtUe-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffdadbef-d624-4362-91fc-fa10d99df10a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import librosa\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get emotion categories (remove calm it has been grouped with neutral prior)\n",
        "path = 'drive/MyDrive/processed_emotion_dataset/'\n",
        "sub_folder = glob.glob(path + '*')\n",
        "sub_folder.remove('drive/MyDrive/processed_emotion_dataset/calm')\n",
        "emotions = [i.split('/')[3] for i in sub_folder]\n",
        "emotions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SwSDrrZt56p",
        "outputId": "3a7e2a9a-515a-4407-f3a0-1cbad7fa656e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['neutral', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print count of each file\n",
        "for cat in emotions:\n",
        "  f_list = glob.glob(path + cat + '/*')\n",
        "  print('Emotion category: \\'{}\\' Count: {}'.format(cat,len(f_list)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izTOkRB_yByp",
        "outputId": "da1dc4d6-0a25-4fb7-d420-345e98859f94"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emotion category: 'neutral' Count: 1619\n",
            "Emotion category: 'happy' Count: 1739\n",
            "Emotion category: 'sad' Count: 1739\n",
            "Emotion category: 'angry' Count: 1739\n",
            "Emotion category: 'fearful' Count: 1739\n",
            "Emotion category: 'disgust' Count: 1739\n",
            "Emotion category: 'surprised' Count: 469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Melspect Dataset Class"
      ],
      "metadata": {
        "id": "qmgTd0RHh4hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class audiodata(Dataset):\n",
        "  def __init__(self, file_paths, n_mels=128):\n",
        "    self.mels = n_mels\n",
        "    self.file_paths = file_paths\n",
        "    self.data = []\n",
        "    for f_name in file_paths:\n",
        "      emotion_category = f_name.split('/')[-1].split('_')[0]\n",
        "      self.data.append([f_name, emotion_category])\n",
        "    self.class_dict = {\"neutral\": 0,\n",
        "                    \"happy\": 1,\n",
        "                    \"sad\": 2,\n",
        "                    \"angry\": 3,\n",
        "                    \"fearful\": 4,\n",
        "                    \"disgust\": 5,\n",
        "                    \"surprised\": 6 \n",
        "                    }\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    audio_path, emotion_class = self.data[index]\n",
        "    x, sr = librosa.load(audio_path)\n",
        "    melspect = librosa.feature.melspectrogram(y=x, sr=sr, n_mels=128, pad_mode='constant')\n",
        "    pad_crop_length = 128 - melspect.shape[1]\n",
        "    if pad_crop_length > 0:\n",
        "      melspect = np.pad(melspect, [(0,0),(0,pad_crop_length)], mode='constant')\n",
        "    if pad_crop_length < 0:\n",
        "      melspect = melspect[:,0:128]\n",
        "    melspect = melspect[np.newaxis,:,:]\n",
        "    # get emotion class code\n",
        "    class_code = self.class_dict[emotion_class]\n",
        "    audio_tensor = torch.from_numpy(melspect)\n",
        "    \n",
        "    return audio_tensor, class_code"
      ],
      "metadata": {
        "id": "bKTyhOOjh34H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Training, Validation, Test Split\n"
      ],
      "metadata": {
        "id": "zwwnUi2Hk1uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "import enumerate"
      ],
      "metadata": {
        "id": "LkzxyDkiB1UI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "ac1e570d-0b2d-4903-83f9-8612feb3c50b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-f26edd53c317>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'enumerate'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get paths for each emotion category\n",
        "sub_folder"
      ],
      "metadata": {
        "id": "U2WvD_Polacl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c431bd4b-269d-416d-f679-7dca8ccaa568"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['drive/MyDrive/processed_emotion_dataset/neutral',\n",
              " 'drive/MyDrive/processed_emotion_dataset/happy',\n",
              " 'drive/MyDrive/processed_emotion_dataset/sad',\n",
              " 'drive/MyDrive/processed_emotion_dataset/angry',\n",
              " 'drive/MyDrive/processed_emotion_dataset/fearful',\n",
              " 'drive/MyDrive/processed_emotion_dataset/disgust',\n",
              " 'drive/MyDrive/processed_emotion_dataset/surprised']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get all files\n",
        "file_list = []\n",
        "for folder in sub_folder:\n",
        "  file_list += glob.glob(folder + \"/*\")"
      ],
      "metadata": {
        "id": "MkxgUHR7CiFK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataset\n",
        "dataset = audiodata(file_list)"
      ],
      "metadata": {
        "id": "AWoBLdDXDMkC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_dataset(dataset, val_split=0.20):\n",
        "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
        "    datasets = {}\n",
        "    datasets['train'] = Subset(dataset, train_idx)\n",
        "    datasets['val'] = Subset(dataset, val_idx)\n",
        "    return datasets"
      ],
      "metadata": {
        "id": "e_iLvWr9DQAS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = train_val_dataset(dataset)\n",
        "print(len(datasets['train']))\n",
        "print(len(datasets['val']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8F6GQbZHDOs",
        "outputId": "d3a54809-60d4-4653-eac0-7fda23422b4b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8626\n",
            "2157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = {x:DataLoader(datasets[x],32, shuffle=True, num_workers=2) for x in ['train','val']}\n",
        "x,y = next(iter(dataloaders['train']))\n",
        "print(x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMEX5jvHHETF",
        "outputId": "4ca772bc-1318-46a1-80f9-79f9061af923"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 128, 128]) torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use cuda\n",
        "torch.cuda.get_device_name(0)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "ZjpB_7X4HZGw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create NN model\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class AudioEmotionClassifier(nn.Module):\n",
        "  def __init__(self, debug=False):\n",
        "    super(AudioEmotionClassifier, self).__init__()\n",
        "    self.debug=debug\n",
        "\n",
        "    # first convolutional layer\n",
        "    self.conv1 = nn.Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
        "    self.bn1 = nn.BatchNorm2d(8)\n",
        "\n",
        "    # second convolutional layer\n",
        "    self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=(2, 2), padding=(1, 1))\n",
        "    self.bn2 = nn.BatchNorm2d(18)\n",
        "    self.pool2 = nn.MaxPool2d(2, stride=2)\n",
        "\n",
        "\n",
        "    # third convolutional layer\n",
        "    self.conv3 = nn.Conv2d(16, 32, kernel_size=3, stride=(2, 2), padding=(1, 1))\n",
        "    self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "    # fourth convolutional layer\n",
        "    self.conv4 = nn.Conv2d(32, 64, kernel_size=3, stride=(2, 2), padding=(1, 1))\n",
        "    self.bn4 = nn.BatchNorm2d(64)\n",
        "    self.pool4 = nn.AvgPool2d(2)\n",
        "\n",
        "    # fully connected layer\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.fc = nn.linear(64,10)\n",
        "\n",
        "    # Softmax layer\n",
        "    self.output = nn.LogSoftmax(dim=1)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # make this prettier\n",
        "    # forward pass\n",
        "    x = self.bn1(F.relu(self.conv1(x)))\n",
        "\n",
        "    x = self.pool2(self.bn2(F.relu(self.conv2(x))))\n",
        "\n",
        "    x = self.bn3(F.relu(self.conv3(x)))\n",
        "\n",
        "    x = self.pool4(self.bn4(F.relu(self.conv4(x))))\n",
        "\n",
        "    x = self.flatten(x)\n",
        "\n",
        "    x = F.relu(self.fc(x))\n",
        "\n",
        "    x = self.output(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "BduZgNw0c_dF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "  for i, (X, y) in enumarate(dataloader):\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    # backprop\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "YSAEaE3xLH3J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}